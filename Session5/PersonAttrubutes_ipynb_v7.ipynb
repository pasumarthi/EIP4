{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonAttrubutes.ipynb_v7",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasumarthi/EIP4/blob/master/Session5/PersonAttrubutes_ipynb_v7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "50f1fe69-97f5-4be7-83c8-e3b40ae9f5b3"
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/EIP3.0/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34msaved_models\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "ae6dac02-05da-483d-dbf9-82d56bbb6b7a"
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "outputId": "152d7d92-0c73-4316-f715-f564e254f6a2"
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, aug_list=[], incl_orig=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "        self.aug_list = aug_list\n",
        "        self.incl_orig = incl_orig\n",
        "        self.orig_len = int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.incl_orig:\n",
        "            delta = 1\n",
        "        else:\n",
        "            delta = 0\n",
        "        return self.orig_len * (len(self.aug_list) + delta)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if not self.incl_orig :\n",
        "            index += self.orig_len - 1\n",
        "\n",
        "        if index > self.orig_len - 1:\n",
        "            aug = self.aug_list[index // self.orig_len - 1]\n",
        "            index %= self.orig_len\n",
        "        else:\n",
        "            aug = None\n",
        "\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        \n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        images = images / 255.0\n",
        "#        print(images)      \n",
        "        \n",
        "        if aug is not None:\n",
        "            images = aug.flow(images, shuffle=False).next()\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        \n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "5419becf-494d-4135-f67f-6550b0de31c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "6758e420-5983-495c-fc4b-c5da2939869e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>721</th>\n",
              "      <td>resized/722.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6195</th>\n",
              "      <td>resized/6196.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10986</th>\n",
              "      <td>resized/10988.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4178</th>\n",
              "      <td>resized/4179.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4223</th>\n",
              "      <td>resized/4224.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "721      resized/722.jpg              0  ...                        1              0\n",
              "6195    resized/6196.jpg              0  ...                        0              1\n",
              "10986  resized/10988.jpg              0  ...                        1              0\n",
              "4178    resized/4179.jpg              0  ...                        1              0\n",
              "4223    resized/4224.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nrncXX7zR9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp6YIJtCzaAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "   return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "   \n",
        "lr_scheduler = LearningRateScheduler(scheduler)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr5V2mcl9f8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Model\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'PersonAttribute%s_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             mode='min',\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                                cooldown=0,\n",
        "                                patience=5,\n",
        "                                min_lr=0.5e-6)\n",
        "\n",
        "#callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint, lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBNOdPYmzohV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    batch_size=32, \n",
        "    aug_list=[\n",
        "        ImageDataGenerator(horizontal_flip=True,vertical_flip=True,shear_range=0.2,\n",
        "        zoom_range=0.2,preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "          ],\n",
        "    incl_orig=False,\n",
        ")\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "140e67a3-e1d9-4244-da58-ce2474e4f544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = keras.applications.resnet_v2.ResNet50V2(\n",
        "    weights=None, \n",
        "    include_top=False, \n",
        "    pooling='avg',\n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "#neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    #neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(in_layer)\n",
        "    #neck = Dropout(0.3)(neck)\n",
        "    #neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"sigmoid\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHArdhFcbfqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fcf6c7f-b84b-4c1d-c8ef-0077194dc1de"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 56, 56, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 56, 56, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 56, 56, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 28, 28, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 28, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 28, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 28, 28, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 28, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 28, 28, 256)  0           max_pooling2d_13[0][0]           \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 28, 28, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 28, 28, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 28, 28, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 14, 14, 128)  147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 14, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 14, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 14, 14, 512)  0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 14, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 14, 14, 512)  0           max_pooling2d_14[0][0]           \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 14, 14, 512)  2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 14, 14, 512)  0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 7, 7, 1024)   0           max_pooling2d_15[0][0]           \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 7, 7, 1024)   4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 7, 7, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           post_relu[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 512)          1049088     avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 128)          65664       dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_30[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 25,142,683\n",
            "Trainable params: 25,097,243\n",
            "Non-trainable params: 45,440\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90dd3506-9c3e-4fc1-9ff2-524cf20e6778"
      },
      "source": [
        "from keras.optimizers import Adam \n",
        "#losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 2.0, \"age_output\": 2.0,\"emotion_output\":1.5,\"footwear_output\":1,\"weight_output\":1.5,\"pose_output\":2,\"bag_output\":1}\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=sgd,\n",
        "    loss=\"binary_crossentropy\", \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "'''model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.01),\n",
        "              metrics=['accuracy'])'''"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"model.compile(loss='categorical_crossentropy',\\n              optimizer=Adam(0.01),\\n              metrics=['accuracy'])\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3e2ddcb-a637-4722-a14d-b2bd0faca7e0"
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=5, \n",
        "    epochs=50,\n",
        "    verbose=1,callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.2336 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.5792 - age_output_loss: 0.4599 - weight_output_loss: 0.4275 - bag_output_loss: 0.5514 - footwear_output_loss: 0.5941 - pose_output_loss: 0.5496 - emotion_output_loss: 0.3920 - gender_output_acc: 0.5634 - image_quality_output_acc: 0.6973 - age_output_acc: 0.7989 - weight_output_acc: 0.8127 - bag_output_acc: 0.7064 - footwear_output_acc: 0.6756 - pose_output_acc: 0.7443 - emotion_output_acc: 0.8531\n",
            "\n",
            "360/360 [==============================] - 213s 591ms/step - loss: 6.2328 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.5793 - age_output_loss: 0.4598 - weight_output_loss: 0.4271 - bag_output_loss: 0.5513 - footwear_output_loss: 0.5943 - pose_output_loss: 0.5497 - emotion_output_loss: 0.3918 - gender_output_acc: 0.5632 - image_quality_output_acc: 0.6973 - age_output_acc: 0.7989 - weight_output_acc: 0.8130 - bag_output_acc: 0.7065 - footwear_output_acc: 0.6756 - pose_output_acc: 0.7442 - emotion_output_acc: 0.8532 - val_loss: 6.1830 - val_gender_output_loss: 0.6681 - val_image_quality_output_loss: 0.5919 - val_age_output_loss: 0.4510 - val_weight_output_loss: 0.4159 - val_bag_output_loss: 0.5504 - val_footwear_output_loss: 0.5900 - val_pose_output_loss: 0.5535 - val_emotion_output_loss: 0.3719 - val_gender_output_acc: 0.6056 - val_image_quality_output_acc: 0.6897 - val_age_output_acc: 0.7982 - val_weight_output_acc: 0.8211 - val_bag_output_acc: 0.7041 - val_footwear_output_acc: 0.6855 - val_pose_output_acc: 0.7376 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00001: val_loss improved from 11.99107 to 6.18297, saving model to /content/saved_models/PersonAttribute%s_model.001.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 6.1592 - gender_output_loss: 0.6702 - image_quality_output_loss: 0.5754 - age_output_loss: 0.4561 - weight_output_loss: 0.4212 - bag_output_loss: 0.5473 - footwear_output_loss: 0.5779 - pose_output_loss: 0.5440 - emotion_output_loss: 0.3874 - gender_output_acc: 0.5898 - image_quality_output_acc: 0.7013 - age_output_acc: 0.8001 - weight_output_acc: 0.8172 - bag_output_acc: 0.7093 - footwear_output_acc: 0.6960 - pose_output_acc: 0.7465 - emotion_output_acc: 0.8546 - val_loss: 6.1239 - val_gender_output_loss: 0.6636 - val_image_quality_output_loss: 0.5792 - val_age_output_loss: 0.4501 - val_weight_output_loss: 0.4146 - val_bag_output_loss: 0.5456 - val_footwear_output_loss: 0.5727 - val_pose_output_loss: 0.5511 - val_emotion_output_loss: 0.3728 - val_gender_output_acc: 0.6043 - val_image_quality_output_acc: 0.6941 - val_age_output_acc: 0.7980 - val_weight_output_acc: 0.8211 - val_bag_output_acc: 0.7048 - val_footwear_output_acc: 0.6999 - val_pose_output_acc: 0.7376 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.18297 to 6.12386, saving model to /content/saved_models/PersonAttribute%s_model.002.h5\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 189s 524ms/step - loss: 6.1364 - gender_output_loss: 0.6671 - image_quality_output_loss: 0.5746 - age_output_loss: 0.4550 - weight_output_loss: 0.4205 - bag_output_loss: 0.5454 - footwear_output_loss: 0.5695 - pose_output_loss: 0.5422 - emotion_output_loss: 0.3867 - gender_output_acc: 0.5902 - image_quality_output_acc: 0.7025 - age_output_acc: 0.8000 - weight_output_acc: 0.8173 - bag_output_acc: 0.7091 - footwear_output_acc: 0.7031 - pose_output_acc: 0.7465 - emotion_output_acc: 0.8544 - val_loss: 6.1104 - val_gender_output_loss: 0.6677 - val_image_quality_output_loss: 0.5785 - val_age_output_loss: 0.4458 - val_weight_output_loss: 0.4146 - val_bag_output_loss: 0.5453 - val_footwear_output_loss: 0.5700 - val_pose_output_loss: 0.5494 - val_emotion_output_loss: 0.3722 - val_gender_output_acc: 0.5796 - val_image_quality_output_acc: 0.6930 - val_age_output_acc: 0.8001 - val_weight_output_acc: 0.8201 - val_bag_output_acc: 0.7053 - val_footwear_output_acc: 0.7001 - val_pose_output_acc: 0.7374 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00003: val_loss improved from 6.12386 to 6.11038, saving model to /content/saved_models/PersonAttribute%s_model.003.h5\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 187s 518ms/step - loss: 6.1183 - gender_output_loss: 0.6600 - image_quality_output_loss: 0.5739 - age_output_loss: 0.4544 - weight_output_loss: 0.4197 - bag_output_loss: 0.5442 - footwear_output_loss: 0.5655 - pose_output_loss: 0.5415 - emotion_output_loss: 0.3863 - gender_output_acc: 0.5974 - image_quality_output_acc: 0.7042 - age_output_acc: 0.8000 - weight_output_acc: 0.8173 - bag_output_acc: 0.7108 - footwear_output_acc: 0.7063 - pose_output_acc: 0.7468 - emotion_output_acc: 0.8545 - val_loss: 6.0859 - val_gender_output_loss: 0.6524 - val_image_quality_output_loss: 0.5788 - val_age_output_loss: 0.4465 - val_weight_output_loss: 0.4143 - val_bag_output_loss: 0.5432 - val_footwear_output_loss: 0.5650 - val_pose_output_loss: 0.5472 - val_emotion_output_loss: 0.3727 - val_gender_output_acc: 0.6192 - val_image_quality_output_acc: 0.6922 - val_age_output_acc: 0.8000 - val_weight_output_acc: 0.8202 - val_bag_output_acc: 0.7060 - val_footwear_output_acc: 0.7031 - val_pose_output_acc: 0.7376 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00004: val_loss improved from 6.11038 to 6.08591, saving model to /content/saved_models/PersonAttribute%s_model.004.h5\n",
            "Epoch 5/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.0996 - gender_output_loss: 0.6561 - image_quality_output_loss: 0.5729 - age_output_loss: 0.4538 - weight_output_loss: 0.4193 - bag_output_loss: 0.5423 - footwear_output_loss: 0.5599 - pose_output_loss: 0.5407 - emotion_output_loss: 0.3850 - gender_output_acc: 0.6047 - image_quality_output_acc: 0.7032 - age_output_acc: 0.8000 - weight_output_acc: 0.8172 - bag_output_acc: 0.7113 - footwear_output_acc: 0.7073 - pose_output_acc: 0.7462 - emotion_output_acc: 0.8548\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 6.0991 - gender_output_loss: 0.6559 - image_quality_output_loss: 0.5727 - age_output_loss: 0.4539 - weight_output_loss: 0.4193 - bag_output_loss: 0.5421 - footwear_output_loss: 0.5600 - pose_output_loss: 0.5407 - emotion_output_loss: 0.3852 - gender_output_acc: 0.6047 - image_quality_output_acc: 0.7034 - age_output_acc: 0.8000 - weight_output_acc: 0.8171 - bag_output_acc: 0.7117 - footwear_output_acc: 0.7072 - pose_output_acc: 0.7462 - emotion_output_acc: 0.8548 - val_loss: 6.0848 - val_gender_output_loss: 0.6521 - val_image_quality_output_loss: 0.5778 - val_age_output_loss: 0.4459 - val_weight_output_loss: 0.4129 - val_bag_output_loss: 0.5412 - val_footwear_output_loss: 0.5684 - val_pose_output_loss: 0.5480 - val_emotion_output_loss: 0.3735 - val_gender_output_acc: 0.6300 - val_image_quality_output_acc: 0.6922 - val_age_output_acc: 0.8001 - val_weight_output_acc: 0.8211 - val_bag_output_acc: 0.7098 - val_footwear_output_acc: 0.7009 - val_pose_output_acc: 0.7381 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00005: val_loss improved from 6.08591 to 6.08479, saving model to /content/saved_models/PersonAttribute%s_model.005.h5\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 186s 518ms/step - loss: 6.0744 - gender_output_loss: 0.6491 - image_quality_output_loss: 0.5712 - age_output_loss: 0.4530 - weight_output_loss: 0.4182 - bag_output_loss: 0.5407 - footwear_output_loss: 0.5539 - pose_output_loss: 0.5384 - emotion_output_loss: 0.3855 - gender_output_acc: 0.6161 - image_quality_output_acc: 0.7036 - age_output_acc: 0.7998 - weight_output_acc: 0.8174 - bag_output_acc: 0.7124 - footwear_output_acc: 0.7132 - pose_output_acc: 0.7466 - emotion_output_acc: 0.8545 - val_loss: 6.1176 - val_gender_output_loss: 0.6669 - val_image_quality_output_loss: 0.5767 - val_age_output_loss: 0.4459 - val_weight_output_loss: 0.4156 - val_bag_output_loss: 0.5493 - val_footwear_output_loss: 0.5819 - val_pose_output_loss: 0.5467 - val_emotion_output_loss: 0.3716 - val_gender_output_acc: 0.5885 - val_image_quality_output_acc: 0.6930 - val_age_output_acc: 0.7998 - val_weight_output_acc: 0.8211 - val_bag_output_acc: 0.6904 - val_footwear_output_acc: 0.6964 - val_pose_output_acc: 0.7376 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.08479\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 186s 518ms/step - loss: 6.0744 - gender_output_loss: 0.6491 - image_quality_output_loss: 0.5712 - age_output_loss: 0.4530 - weight_output_loss: 0.4182 - bag_output_loss: 0.5407 - footwear_output_loss: 0.5539 - pose_output_loss: 0.5384 - emotion_output_loss: 0.3855 - gender_output_acc: 0.6161 - image_quality_output_acc: 0.7036 - age_output_acc: 0.7998 - weight_output_acc: 0.8174 - bag_output_acc: 0.7124 - footwear_output_acc: 0.7132 - pose_output_acc: 0.7466 - emotion_output_acc: 0.8545 - val_loss: 6.1176 - val_gender_output_loss: 0.6669 - val_image_quality_output_loss: 0.5767 - val_age_output_loss: 0.4459 - val_weight_output_loss: 0.4156 - val_bag_output_loss: 0.5493 - val_footwear_output_loss: 0.5819 - val_pose_output_loss: 0.5467 - val_emotion_output_loss: 0.3716 - val_gender_output_acc: 0.5885 - val_image_quality_output_acc: 0.6930 - val_age_output_acc: 0.7998 - val_weight_output_acc: 0.8211 - val_bag_output_acc: 0.6904 - val_footwear_output_acc: 0.6964 - val_pose_output_acc: 0.7376 - val_emotion_output_acc: 0.8642\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 6.0511 - gender_output_loss: 0.6360 - image_quality_output_loss: 0.5707 - age_output_loss: 0.4524 - weight_output_loss: 0.4182 - bag_output_loss: 0.5379 - footwear_output_loss: 0.5524 - pose_output_loss: 0.5371 - emotion_output_loss: 0.3848 - gender_output_acc: 0.6327 - image_quality_output_acc: 0.7041 - age_output_acc: 0.8000 - weight_output_acc: 0.8176 - bag_output_acc: 0.7164 - footwear_output_acc: 0.7146 - pose_output_acc: 0.7470 - emotion_output_acc: 0.8545 - val_loss: 6.2769 - val_gender_output_loss: 0.7236 - val_image_quality_output_loss: 0.5861 - val_age_output_loss: 0.4519 - val_weight_output_loss: 0.4184 - val_bag_output_loss: 0.5645 - val_footwear_output_loss: 0.6295 - val_pose_output_loss: 0.5469 - val_emotion_output_loss: 0.3746 - val_gender_output_acc: 0.4952 - val_image_quality_output_acc: 0.6925 - val_age_output_acc: 0.7999 - val_weight_output_acc: 0.8211 - val_bag_output_acc: 0.6606 - val_footwear_output_acc: 0.6715 - val_pose_output_acc: 0.7379 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.08479\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 6.0302 - gender_output_loss: 0.6320 - image_quality_output_loss: 0.5688 - age_output_loss: 0.4521 - weight_output_loss: 0.4175 - bag_output_loss: 0.5369 - footwear_output_loss: 0.5478 - pose_output_loss: 0.5345 - emotion_output_loss: 0.3843 - gender_output_acc: 0.6376 - image_quality_output_acc: 0.7045 - age_output_acc: 0.7998 - weight_output_acc: 0.8171 - bag_output_acc: 0.7192 - footwear_output_acc: 0.7188 - pose_output_acc: 0.7466 - emotion_output_acc: 0.8546 - val_loss: 6.3405 - val_gender_output_loss: 0.6697 - val_image_quality_output_loss: 0.5985 - val_age_output_loss: 0.4493 - val_weight_output_loss: 0.4238 - val_bag_output_loss: 0.5633 - val_footwear_output_loss: 0.6971 - val_pose_output_loss: 0.5559 - val_emotion_output_loss: 0.3781 - val_gender_output_acc: 0.6331 - val_image_quality_output_acc: 0.6900 - val_age_output_acc: 0.8001 - val_weight_output_acc: 0.8109 - val_bag_output_acc: 0.7067 - val_footwear_output_acc: 0.6302 - val_pose_output_acc: 0.7379 - val_emotion_output_acc: 0.8628\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.08479\n",
            "Epoch 9/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.0074 - gender_output_loss: 0.6231 - image_quality_output_loss: 0.5660 - age_output_loss: 0.4519 - weight_output_loss: 0.4177 - bag_output_loss: 0.5360 - footwear_output_loss: 0.5444 - pose_output_loss: 0.5331 - emotion_output_loss: 0.3837 - gender_output_acc: 0.6435 - image_quality_output_acc: 0.7040 - age_output_acc: 0.8002 - weight_output_acc: 0.8173 - bag_output_acc: 0.7196 - footwear_output_acc: 0.7203 - pose_output_acc: 0.7474 - emotion_output_acc: 0.8545\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.08479\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 6.0067 - gender_output_loss: 0.6229 - image_quality_output_loss: 0.5660 - age_output_loss: 0.4518 - weight_output_loss: 0.4176 - bag_output_loss: 0.5359 - footwear_output_loss: 0.5443 - pose_output_loss: 0.5330 - emotion_output_loss: 0.3838 - gender_output_acc: 0.6440 - image_quality_output_acc: 0.7039 - age_output_acc: 0.8002 - weight_output_acc: 0.8173 - bag_output_acc: 0.7196 - footwear_output_acc: 0.7202 - pose_output_acc: 0.7475 - emotion_output_acc: 0.8544 - val_loss: 6.0369 - val_gender_output_loss: 0.6471 - val_image_quality_output_loss: 0.5710 - val_age_output_loss: 0.4479 - val_weight_output_loss: 0.4165 - val_bag_output_loss: 0.5415 - val_footwear_output_loss: 0.5457 - val_pose_output_loss: 0.5404 - val_emotion_output_loss: 0.3730 - val_gender_output_acc: 0.6426 - val_image_quality_output_acc: 0.6909 - val_age_output_acc: 0.8000 - val_weight_output_acc: 0.8146 - val_bag_output_acc: 0.7109 - val_footwear_output_acc: 0.7300 - val_pose_output_acc: 0.7372 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00009: val_loss improved from 6.08479 to 6.03693, saving model to /content/saved_models/PersonAttribute%s_model.009.h5\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 186s 518ms/step - loss: 5.9861 - gender_output_loss: 0.6175 - image_quality_output_loss: 0.5636 - age_output_loss: 0.4517 - weight_output_loss: 0.4171 - bag_output_loss: 0.5332 - footwear_output_loss: 0.5413 - pose_output_loss: 0.5309 - emotion_output_loss: 0.3840 - gender_output_acc: 0.6497 - image_quality_output_acc: 0.7030 - age_output_acc: 0.7998 - weight_output_acc: 0.8172 - bag_output_acc: 0.7214 - footwear_output_acc: 0.7256 - pose_output_acc: 0.7478 - emotion_output_acc: 0.8544 - val_loss: 6.2230 - val_gender_output_loss: 0.6533 - val_image_quality_output_loss: 0.6131 - val_age_output_loss: 0.4471 - val_weight_output_loss: 0.4165 - val_bag_output_loss: 0.5434 - val_footwear_output_loss: 0.6172 - val_pose_output_loss: 0.5490 - val_emotion_output_loss: 0.3774 - val_gender_output_acc: 0.6064 - val_image_quality_output_acc: 0.6888 - val_age_output_acc: 0.7996 - val_weight_output_acc: 0.8211 - val_bag_output_acc: 0.7061 - val_footwear_output_acc: 0.6769 - val_pose_output_acc: 0.7381 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.03693\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 5.9650 - gender_output_loss: 0.6126 - image_quality_output_loss: 0.5604 - age_output_loss: 0.4508 - weight_output_loss: 0.4170 - bag_output_loss: 0.5324 - footwear_output_loss: 0.5404 - pose_output_loss: 0.5281 - emotion_output_loss: 0.3837 - gender_output_acc: 0.6608 - image_quality_output_acc: 0.7038 - age_output_acc: 0.8000 - weight_output_acc: 0.8180 - bag_output_acc: 0.7198 - footwear_output_acc: 0.7233 - pose_output_acc: 0.7494 - emotion_output_acc: 0.8544 - val_loss: 6.0007 - val_gender_output_loss: 0.6222 - val_image_quality_output_loss: 0.5770 - val_age_output_loss: 0.4442 - val_weight_output_loss: 0.4134 - val_bag_output_loss: 0.5367 - val_footwear_output_loss: 0.5528 - val_pose_output_loss: 0.5363 - val_emotion_output_loss: 0.3693 - val_gender_output_acc: 0.6477 - val_image_quality_output_acc: 0.6897 - val_age_output_acc: 0.8001 - val_weight_output_acc: 0.8207 - val_bag_output_acc: 0.7157 - val_footwear_output_acc: 0.7181 - val_pose_output_acc: 0.7392 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00011: val_loss improved from 6.03693 to 6.00069, saving model to /content/saved_models/PersonAttribute%s_model.011.h5\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 5.9406 - gender_output_loss: 0.6037 - image_quality_output_loss: 0.5575 - age_output_loss: 0.4505 - weight_output_loss: 0.4161 - bag_output_loss: 0.5303 - footwear_output_loss: 0.5369 - pose_output_loss: 0.5269 - emotion_output_loss: 0.3838 - gender_output_acc: 0.6614 - image_quality_output_acc: 0.7041 - age_output_acc: 0.7998 - weight_output_acc: 0.8174 - bag_output_acc: 0.7223 - footwear_output_acc: 0.7277 - pose_output_acc: 0.7494 - emotion_output_acc: 0.8545 - val_loss: 6.2409 - val_gender_output_loss: 0.6467 - val_image_quality_output_loss: 0.6126 - val_age_output_loss: 0.4507 - val_weight_output_loss: 0.4214 - val_bag_output_loss: 0.5602 - val_footwear_output_loss: 0.6331 - val_pose_output_loss: 0.5356 - val_emotion_output_loss: 0.3808 - val_gender_output_acc: 0.6494 - val_image_quality_output_acc: 0.6858 - val_age_output_acc: 0.8000 - val_weight_output_acc: 0.8140 - val_bag_output_acc: 0.7107 - val_footwear_output_acc: 0.6665 - val_pose_output_acc: 0.7421 - val_emotion_output_acc: 0.8640\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.00069\n",
            "Epoch 13/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.9192 - gender_output_loss: 0.5974 - image_quality_output_loss: 0.5561 - age_output_loss: 0.4502 - weight_output_loss: 0.4158 - bag_output_loss: 0.5298 - footwear_output_loss: 0.5345 - pose_output_loss: 0.5237 - emotion_output_loss: 0.3825 - gender_output_acc: 0.6699 - image_quality_output_acc: 0.7038 - age_output_acc: 0.7997 - weight_output_acc: 0.8181 - bag_output_acc: 0.7218 - footwear_output_acc: 0.7299 - pose_output_acc: 0.7504 - emotion_output_acc: 0.8545 - val_loss: 5.9992 - val_gender_output_loss: 0.6168 - val_image_quality_output_loss: 0.5891 - val_age_output_loss: 0.4465 - val_weight_output_loss: 0.4126 - val_bag_output_loss: 0.5416 - val_footwear_output_loss: 0.5409 - val_pose_output_loss: 0.5268 - val_emotion_output_loss: 0.3708 - val_gender_output_acc: 0.6537 - val_image_quality_output_acc: 0.6862 - val_age_output_acc: 0.8005 - val_weight_output_acc: 0.8201 - val_bag_output_acc: 0.7041 - val_footwear_output_acc: 0.7218 - val_pose_output_acc: 0.7446 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00013: val_loss improved from 6.00069 to 5.99923, saving model to /content/saved_models/PersonAttribute%s_model.013.h5\n",
            "Epoch 14/50\n",
            "360/360 [==============================] - 188s 522ms/step - loss: 5.9030 - gender_output_loss: 0.5953 - image_quality_output_loss: 0.5546 - age_output_loss: 0.4496 - weight_output_loss: 0.4158 - bag_output_loss: 0.5271 - footwear_output_loss: 0.5337 - pose_output_loss: 0.5205 - emotion_output_loss: 0.3825 - gender_output_acc: 0.6727 - image_quality_output_acc: 0.7036 - age_output_acc: 0.7999 - weight_output_acc: 0.8178 - bag_output_acc: 0.7284 - footwear_output_acc: 0.7317 - pose_output_acc: 0.7519 - emotion_output_acc: 0.8544 - val_loss: 6.0938 - val_gender_output_loss: 0.6945 - val_image_quality_output_loss: 0.5663 - val_age_output_loss: 0.4470 - val_weight_output_loss: 0.4154 - val_bag_output_loss: 0.5561 - val_footwear_output_loss: 0.5643 - val_pose_output_loss: 0.5361 - val_emotion_output_loss: 0.3715 - val_gender_output_acc: 0.5759 - val_image_quality_output_acc: 0.6905 - val_age_output_acc: 0.7998 - val_weight_output_acc: 0.8206 - val_bag_output_acc: 0.6761 - val_footwear_output_acc: 0.7219 - val_pose_output_acc: 0.7411 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 5.99923\n",
            "Epoch 15/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.8842 - gender_output_loss: 0.5879 - image_quality_output_loss: 0.5527 - age_output_loss: 0.4491 - weight_output_loss: 0.4153 - bag_output_loss: 0.5264 - footwear_output_loss: 0.5338 - pose_output_loss: 0.5186 - emotion_output_loss: 0.3816 - gender_output_acc: 0.6774 - image_quality_output_acc: 0.7052 - age_output_acc: 0.7999 - weight_output_acc: 0.8185 - bag_output_acc: 0.7265 - footwear_output_acc: 0.7287 - pose_output_acc: 0.7519 - emotion_output_acc: 0.8546Epoch 15/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.8846 - gender_output_loss: 0.5878 - image_quality_output_loss: 0.5527 - age_output_loss: 0.4490 - weight_output_loss: 0.4153 - bag_output_loss: 0.5264 - footwear_output_loss: 0.5339 - pose_output_loss: 0.5186 - emotion_output_loss: 0.3819 - gender_output_acc: 0.6775 - image_quality_output_acc: 0.7053 - age_output_acc: 0.7999 - weight_output_acc: 0.8184 - bag_output_acc: 0.7264 - footwear_output_acc: 0.7286 - pose_output_acc: 0.7519 - emotion_output_acc: 0.8545 - val_loss: 6.0516 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.5640 - val_age_output_loss: 0.4461 - val_weight_output_loss: 0.4162 - val_bag_output_loss: 0.5557 - val_footwear_output_loss: 0.5361 - val_pose_output_loss: 0.5375 - val_emotion_output_loss: 0.3702 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.6935 - val_age_output_acc: 0.8002 - val_weight_output_acc: 0.8212 - val_bag_output_acc: 0.6759 - val_footwear_output_acc: 0.7292 - val_pose_output_acc: 0.7423 - val_emotion_output_acc: 0.8644\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 5.99923\n",
            "Epoch 16/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.8651 - gender_output_loss: 0.5861 - image_quality_output_loss: 0.5514 - age_output_loss: 0.4488 - weight_output_loss: 0.4143 - bag_output_loss: 0.5264 - footwear_output_loss: 0.5281 - pose_output_loss: 0.5147 - emotion_output_loss: 0.3821 - gender_output_acc: 0.6842 - image_quality_output_acc: 0.7068 - age_output_acc: 0.7999 - weight_output_acc: 0.8183 - bag_output_acc: 0.7262 - footwear_output_acc: 0.7350 - pose_output_acc: 0.7565 - emotion_output_acc: 0.8545 - val_loss: 5.9175 - val_gender_output_loss: 0.5802 - val_image_quality_output_loss: 0.5732 - val_age_output_loss: 0.4434 - val_weight_output_loss: 0.4124 - val_bag_output_loss: 0.5342 - val_footwear_output_loss: 0.5440 - val_pose_output_loss: 0.5269 - val_emotion_output_loss: 0.3690 - val_gender_output_acc: 0.6762 - val_image_quality_output_acc: 0.6927 - val_age_output_acc: 0.8001 - val_weight_output_acc: 0.8204 - val_bag_output_acc: 0.7100 - val_footwear_output_acc: 0.7275 - val_pose_output_acc: 0.7471 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00016: val_loss improved from 5.99923 to 5.91750, saving model to /content/saved_models/PersonAttribute%s_model.016.h5\n",
            "Epoch 17/50\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 5.8442 - gender_output_loss: 0.5778 - image_quality_output_loss: 0.5495 - age_output_loss: 0.4489 - weight_output_loss: 0.4143 - bag_output_loss: 0.5249 - footwear_output_loss: 0.5270 - pose_output_loss: 0.5119 - emotion_output_loss: 0.3815 - gender_output_acc: 0.6932 - image_quality_output_acc: 0.7067 - age_output_acc: 0.7999 - weight_output_acc: 0.8191 - bag_output_acc: 0.7288 - footwear_output_acc: 0.7330 - pose_output_acc: 0.7581 - emotion_output_acc: 0.8544 - val_loss: 5.9343 - val_gender_output_loss: 0.6325 - val_image_quality_output_loss: 0.5629 - val_age_output_loss: 0.4454 - val_weight_output_loss: 0.4135 - val_bag_output_loss: 0.5414 - val_footwear_output_loss: 0.5336 - val_pose_output_loss: 0.5169 - val_emotion_output_loss: 0.3709 - val_gender_output_acc: 0.6447 - val_image_quality_output_acc: 0.6957 - val_age_output_acc: 0.8010 - val_weight_output_acc: 0.8212 - val_bag_output_acc: 0.7030 - val_footwear_output_acc: 0.7293 - val_pose_output_acc: 0.7522 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 5.91750\n",
            "Epoch 18/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.8313 - gender_output_loss: 0.5744 - image_quality_output_loss: 0.5497 - age_output_loss: 0.4488 - weight_output_loss: 0.4143 - bag_output_loss: 0.5238 - footwear_output_loss: 0.5307 - pose_output_loss: 0.5058 - emotion_output_loss: 0.3816 - gender_output_acc: 0.6968 - image_quality_output_acc: 0.7075 - age_output_acc: 0.7997 - weight_output_acc: 0.8189 - bag_output_acc: 0.7289 - footwear_output_acc: 0.7329 - pose_output_acc: 0.7582 - emotion_output_acc: 0.8545Epoch 18/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.8310 - gender_output_loss: 0.5743 - image_quality_output_loss: 0.5496 - age_output_loss: 0.4489 - weight_output_loss: 0.4147 - bag_output_loss: 0.5238 - footwear_output_loss: 0.5305 - pose_output_loss: 0.5055 - emotion_output_loss: 0.3816 - gender_output_acc: 0.6970 - image_quality_output_acc: 0.7075 - age_output_acc: 0.7997 - weight_output_acc: 0.8187 - bag_output_acc: 0.7290 - footwear_output_acc: 0.7328 - pose_output_acc: 0.7584 - emotion_output_acc: 0.8545 - val_loss: 5.9546 - val_gender_output_loss: 0.6024 - val_image_quality_output_loss: 0.5835 - val_age_output_loss: 0.4442 - val_weight_output_loss: 0.4118 - val_bag_output_loss: 0.5401 - val_footwear_output_loss: 0.5420 - val_pose_output_loss: 0.5206 - val_emotion_output_loss: 0.3705 - val_gender_output_acc: 0.6749 - val_image_quality_output_acc: 0.6919 - val_age_output_acc: 0.8017 - val_weight_output_acc: 0.8204 - val_bag_output_acc: 0.7184 - val_footwear_output_acc: 0.7287 - val_pose_output_acc: 0.7520 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 5.91750\n",
            "Epoch 19/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.8057 - gender_output_loss: 0.5689 - image_quality_output_loss: 0.5473 - age_output_loss: 0.4487 - weight_output_loss: 0.4142 - bag_output_loss: 0.5244 - footwear_output_loss: 0.5248 - pose_output_loss: 0.5014 - emotion_output_loss: 0.3810 - gender_output_acc: 0.6996 - image_quality_output_acc: 0.7086 - age_output_acc: 0.8000 - weight_output_acc: 0.8185 - bag_output_acc: 0.7296 - footwear_output_acc: 0.7381 - pose_output_acc: 0.7621 - emotion_output_acc: 0.8544360/360 [==============================] - 188s 521ms/step - loss: 5.8310 - gender_output_loss: 0.5743 - image_quality_output_loss: 0.5496 - age_output_loss: 0.4489 - weight_output_loss: 0.4147 - bag_output_loss: 0.5238 - footwear_output_loss: 0.5305 - pose_output_loss: 0.5055 - emotion_output_loss: 0.3816 - gender_output_acc: 0.6970 - image_quality_output_acc: 0.7075 - age_output_acc: 0.7997 - weight_output_acc: 0.8187 - bag_output_acc: 0.7290 - footwear_output_acc: 0.7328 - pose_output_acc: 0.7584 - emotion_output_acc: 0.8545 - val_loss: 5.9546 - val_gender_output_loss: 0.6024 - val_image_quality_output_loss: 0.5835 - val_age_output_loss: 0.4442 - val_weight_output_loss: 0.4118 - val_bag_output_loss: 0.5401 - val_footwear_output_loss: 0.5420 - val_pose_output_loss: 0.5206 - val_emotion_output_loss: 0.3705 - val_gender_output_acc: 0.6749 - val_image_quality_output_acc: 0.6919 - val_age_output_acc: 0.8017 - val_weight_output_acc: 0.8204 - val_bag_output_acc: 0.7184 - val_footwear_output_acc: 0.7287 - val_pose_output_acc: 0.7520 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 19/50\n",
            "360/360 [==============================] - 190s 527ms/step - loss: 5.8052 - gender_output_loss: 0.5687 - image_quality_output_loss: 0.5472 - age_output_loss: 0.4486 - weight_output_loss: 0.4142 - bag_output_loss: 0.5244 - footwear_output_loss: 0.5249 - pose_output_loss: 0.5015 - emotion_output_loss: 0.3809 - gender_output_acc: 0.6998 - image_quality_output_acc: 0.7086 - age_output_acc: 0.8000 - weight_output_acc: 0.8186 - bag_output_acc: 0.7296 - footwear_output_acc: 0.7381 - pose_output_acc: 0.7621 - emotion_output_acc: 0.8545 - val_loss: 5.8280 - val_gender_output_loss: 0.5852 - val_image_quality_output_loss: 0.5622 - val_age_output_loss: 0.4423 - val_weight_output_loss: 0.4120 - val_bag_output_loss: 0.5344 - val_footwear_output_loss: 0.5234 - val_pose_output_loss: 0.5013 - val_emotion_output_loss: 0.3702 - val_gender_output_acc: 0.6832 - val_image_quality_output_acc: 0.6993 - val_age_output_acc: 0.8008 - val_weight_output_acc: 0.8198 - val_bag_output_acc: 0.7151 - val_footwear_output_acc: 0.7431 - val_pose_output_acc: 0.7623 - val_emotion_output_acc: 0.8639\n",
            "\n",
            "Epoch 00019: val_loss improved from 5.91750 to 5.82796, saving model to /content/saved_models/PersonAttribute%s_model.019.h5\n",
            "Epoch 20/50\n",
            "360/360 [==============================] - 187s 521ms/step - loss: 5.7972 - gender_output_loss: 0.5713 - image_quality_output_loss: 0.5471 - age_output_loss: 0.4486 - weight_output_loss: 0.4146 - bag_output_loss: 0.5221 - footwear_output_loss: 0.5258 - pose_output_loss: 0.4975 - emotion_output_loss: 0.3798 - gender_output_acc: 0.6941 - image_quality_output_acc: 0.7085 - age_output_acc: 0.7999 - weight_output_acc: 0.8189 - bag_output_acc: 0.7302 - footwear_output_acc: 0.7377 - pose_output_acc: 0.7633 - emotion_output_acc: 0.8546 - val_loss: 5.9783 - val_gender_output_loss: 0.6637 - val_image_quality_output_loss: 0.5776 - val_age_output_loss: 0.4482 - val_weight_output_loss: 0.4161 - val_bag_output_loss: 0.5506 - val_footwear_output_loss: 0.5322 - val_pose_output_loss: 0.5002 - val_emotion_output_loss: 0.3704 - val_gender_output_acc: 0.6530 - val_image_quality_output_acc: 0.6983 - val_age_output_acc: 0.8001 - val_weight_output_acc: 0.8197 - val_bag_output_acc: 0.7159 - val_footwear_output_acc: 0.7413 - val_pose_output_acc: 0.7651 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.82796\n",
            "Epoch 21/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.7784 - gender_output_loss: 0.5671 - image_quality_output_loss: 0.5459 - age_output_loss: 0.4474 - weight_output_loss: 0.4143 - bag_output_loss: 0.5229 - footwear_output_loss: 0.5234 - pose_output_loss: 0.4933 - emotion_output_loss: 0.3802 - gender_output_acc: 0.7000 - image_quality_output_acc: 0.7109 - age_output_acc: 0.8001 - weight_output_acc: 0.8180 - bag_output_acc: 0.7298 - footwear_output_acc: 0.7368 - pose_output_acc: 0.7664 - emotion_output_acc: 0.8546 - val_loss: 5.8649 - val_gender_output_loss: 0.5983 - val_image_quality_output_loss: 0.5650 - val_age_output_loss: 0.4454 - val_weight_output_loss: 0.4131 - val_bag_output_loss: 0.5363 - val_footwear_output_loss: 0.5335 - val_pose_output_loss: 0.5012 - val_emotion_output_loss: 0.3691 - val_gender_output_acc: 0.6893 - val_image_quality_output_acc: 0.7014 - val_age_output_acc: 0.8004 - val_weight_output_acc: 0.8180 - val_bag_output_acc: 0.7147 - val_footwear_output_acc: 0.7364 - val_pose_output_acc: 0.7641 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.82796\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 5.82796\n",
            "Epoch 22/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.7596 - gender_output_loss: 0.5620 - image_quality_output_loss: 0.5450 - age_output_loss: 0.4482 - weight_output_loss: 0.4138 - bag_output_loss: 0.5224 - footwear_output_loss: 0.5217 - pose_output_loss: 0.4884 - emotion_output_loss: 0.3798 - gender_output_acc: 0.7031 - image_quality_output_acc: 0.7123 - age_output_acc: 0.7999 - weight_output_acc: 0.8186 - bag_output_acc: 0.7315 - footwear_output_acc: 0.7394 - pose_output_acc: 0.7685 - emotion_output_acc: 0.8544\n",
            "Epoch 00021: val_loss did not improve from 5.82796\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 5.7594 - gender_output_loss: 0.5620 - image_quality_output_loss: 0.5452 - age_output_loss: 0.4482 - weight_output_loss: 0.4136 - bag_output_loss: 0.5224 - footwear_output_loss: 0.5217 - pose_output_loss: 0.4883 - emotion_output_loss: 0.3797 - gender_output_acc: 0.7030 - image_quality_output_acc: 0.7120 - age_output_acc: 0.7999 - weight_output_acc: 0.8188 - bag_output_acc: 0.7316 - footwear_output_acc: 0.7394 - pose_output_acc: 0.7685 - emotion_output_acc: 0.8544 - val_loss: 6.1018 - val_gender_output_loss: 0.6503 - val_image_quality_output_loss: 0.6218 - val_age_output_loss: 0.4478 - val_weight_output_loss: 0.4174 - val_bag_output_loss: 0.5520 - val_footwear_output_loss: 0.5580 - val_pose_output_loss: 0.5083 - val_emotion_output_loss: 0.3730 - val_gender_output_acc: 0.6434 - val_image_quality_output_acc: 0.6791 - val_age_output_acc: 0.8004 - val_weight_output_acc: 0.8131 - val_bag_output_acc: 0.7114 - val_footwear_output_acc: 0.7273 - val_pose_output_acc: 0.7584 - val_emotion_output_acc: 0.8643\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 5.82796\n",
            "Epoch 23/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.7517 - gender_output_loss: 0.5588 - image_quality_output_loss: 0.5458 - age_output_loss: 0.4478 - weight_output_loss: 0.4141 - bag_output_loss: 0.5224 - footwear_output_loss: 0.5225 - pose_output_loss: 0.4853 - emotion_output_loss: 0.3793 - gender_output_acc: 0.7035 - image_quality_output_acc: 0.7117 - age_output_acc: 0.8004 - weight_output_acc: 0.8189 - bag_output_acc: 0.7322 - footwear_output_acc: 0.7400 - pose_output_acc: 0.7714 - emotion_output_acc: 0.8546 - val_loss: 5.9421 - val_gender_output_loss: 0.5761 - val_image_quality_output_loss: 0.6095 - val_age_output_loss: 0.4457 - val_weight_output_loss: 0.4135 - val_bag_output_loss: 0.5343 - val_footwear_output_loss: 0.5479 - val_pose_output_loss: 0.4954 - val_emotion_output_loss: 0.3750 - val_gender_output_acc: 0.6993 - val_image_quality_output_acc: 0.6757 - val_age_output_acc: 0.8004 - val_weight_output_acc: 0.8221 - val_bag_output_acc: 0.7172 - val_footwear_output_acc: 0.7243 - val_pose_output_acc: 0.7646 - val_emotion_output_acc: 0.8640\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 5.82796\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 188s 522ms/step - loss: 5.7264 - gender_output_loss: 0.5584 - image_quality_output_loss: 0.5439 - age_output_loss: 0.4476 - weight_output_loss: 0.4139 - bag_output_loss: 0.5222 - footwear_output_loss: 0.5192 - pose_output_loss: 0.4774 - emotion_output_loss: 0.3787 - gender_output_acc: 0.7090 - image_quality_output_acc: 0.7127 - age_output_acc: 0.8002 - weight_output_acc: 0.8181 - bag_output_acc: 0.7345 - footwear_output_acc: 0.7418 - pose_output_acc: 0.7753 - emotion_output_acc: 0.8545 - val_loss: 6.3822 - val_gender_output_loss: 0.6809 - val_image_quality_output_loss: 0.5949 - val_age_output_loss: 0.4495 - val_weight_output_loss: 0.4194 - val_bag_output_loss: 0.5738 - val_footwear_output_loss: 0.5521 - val_pose_output_loss: 0.6471 - val_emotion_output_loss: 0.3756 - val_gender_output_acc: 0.6570 - val_image_quality_output_acc: 0.6885 - val_age_output_acc: 0.8009 - val_weight_output_acc: 0.8129 - val_bag_output_acc: 0.7088 - val_footwear_output_acc: 0.7324 - val_pose_output_acc: 0.6531 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 5.82796\n",
            "Epoch 25/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.7096 - gender_output_loss: 0.5512 - image_quality_output_loss: 0.5436 - age_output_loss: 0.4475 - weight_output_loss: 0.4127 - bag_output_loss: 0.5209 - footwear_output_loss: 0.5175 - pose_output_loss: 0.4757 - emotion_output_loss: 0.3782 - gender_output_acc: 0.7141 - image_quality_output_acc: 0.7145 - age_output_acc: 0.7998 - weight_output_acc: 0.8191 - bag_output_acc: 0.7324 - footwear_output_acc: 0.7428 - pose_output_acc: 0.7774 - emotion_output_acc: 0.8545 - val_loss: 6.0928 - val_gender_output_loss: 0.6421 - val_image_quality_output_loss: 0.6367 - val_age_output_loss: 0.4471 - val_weight_output_loss: 0.4143 - val_bag_output_loss: 0.5418 - val_footwear_output_loss: 0.5691 - val_pose_output_loss: 0.4940 - val_emotion_output_loss: 0.3752 - val_gender_output_acc: 0.6356 - val_image_quality_output_acc: 0.6411 - val_age_output_acc: 0.8002 - val_weight_output_acc: 0.8177 - val_bag_output_acc: 0.7157 - val_footwear_output_acc: 0.7056 - val_pose_output_acc: 0.7710 - val_emotion_output_acc: 0.8640\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 5.82796\n",
            "Epoch 26/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6900 - gender_output_loss: 0.5499 - image_quality_output_loss: 0.5433 - age_output_loss: 0.4474 - weight_output_loss: 0.4131 - bag_output_loss: 0.5191 - footwear_output_loss: 0.5139 - pose_output_loss: 0.4698 - emotion_output_loss: 0.3777 - gender_output_acc: 0.7132 - image_quality_output_acc: 0.7127 - age_output_acc: 0.7999 - weight_output_acc: 0.8188 - bag_output_acc: 0.7316 - footwear_output_acc: 0.7462 - pose_output_acc: 0.7801 - emotion_output_acc: 0.8548\n",
            "Epoch 00025: val_loss did not improve from 5.82796\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 5.6919 - gender_output_loss: 0.5502 - image_quality_output_loss: 0.5435 - age_output_loss: 0.4474 - weight_output_loss: 0.4132 - bag_output_loss: 0.5193 - footwear_output_loss: 0.5141 - pose_output_loss: 0.4699 - emotion_output_loss: 0.3780 - gender_output_acc: 0.7128 - image_quality_output_acc: 0.7126 - age_output_acc: 0.7999 - weight_output_acc: 0.8187 - bag_output_acc: 0.7314 - footwear_output_acc: 0.7460 - pose_output_acc: 0.7799 - emotion_output_acc: 0.8545 - val_loss: 5.9011 - val_gender_output_loss: 0.6622 - val_image_quality_output_loss: 0.5840 - val_age_output_loss: 0.4444 - val_weight_output_loss: 0.4120 - val_bag_output_loss: 0.5351 - val_footwear_output_loss: 0.5369 - val_pose_output_loss: 0.4698 - val_emotion_output_loss: 0.3683 - val_gender_output_acc: 0.6585 - val_image_quality_output_acc: 0.6957 - val_age_output_acc: 0.8005 - val_weight_output_acc: 0.8216 - val_bag_output_acc: 0.7199 - val_footwear_output_acc: 0.7344 - val_pose_output_acc: 0.7834 - val_emotion_output_acc: 0.8635\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 5.82796\n",
            "Epoch 27/50\n",
            "360/360 [==============================] - 188s 522ms/step - loss: 5.6714 - gender_output_loss: 0.5417 - image_quality_output_loss: 0.5428 - age_output_loss: 0.4466 - weight_output_loss: 0.4127 - bag_output_loss: 0.5193 - footwear_output_loss: 0.5156 - pose_output_loss: 0.4653 - emotion_output_loss: 0.3776 - gender_output_acc: 0.7223 - image_quality_output_acc: 0.7157 - age_output_acc: 0.7999 - weight_output_acc: 0.8180 - bag_output_acc: 0.7334 - footwear_output_acc: 0.7451 - pose_output_acc: 0.7792 - emotion_output_acc: 0.8546 - val_loss: 5.7821 - val_gender_output_loss: 0.5461 - val_image_quality_output_loss: 0.5895 - val_age_output_loss: 0.4426 - val_weight_output_loss: 0.4127 - val_bag_output_loss: 0.5266 - val_footwear_output_loss: 0.5166 - val_pose_output_loss: 0.4795 - val_emotion_output_loss: 0.3670 - val_gender_output_acc: 0.7152 - val_image_quality_output_acc: 0.6815 - val_age_output_acc: 0.8002 - val_weight_output_acc: 0.8204 - val_bag_output_acc: 0.7250 - val_footwear_output_acc: 0.7456 - val_pose_output_acc: 0.7742 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00027: val_loss improved from 5.82796 to 5.78213, saving model to /content/saved_models/PersonAttribute%s_model.027.h5\n",
            "Epoch 28/50\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 5.6546 - gender_output_loss: 0.5416 - image_quality_output_loss: 0.5411 - age_output_loss: 0.4469 - weight_output_loss: 0.4131 - bag_output_loss: 0.5175 - footwear_output_loss: 0.5111 - pose_output_loss: 0.4610 - emotion_output_loss: 0.3777 - gender_output_acc: 0.7216 - image_quality_output_acc: 0.7154 - age_output_acc: 0.8001 - weight_output_acc: 0.8193 - bag_output_acc: 0.7375 - footwear_output_acc: 0.7480 - pose_output_acc: 0.7830 - emotion_output_acc: 0.8544 - val_loss: 5.7151 - val_gender_output_loss: 0.5506 - val_image_quality_output_loss: 0.5633 - val_age_output_loss: 0.4426 - val_weight_output_loss: 0.4123 - val_bag_output_loss: 0.5402 - val_footwear_output_loss: 0.5281 - val_pose_output_loss: 0.4588 - val_emotion_output_loss: 0.3655 - val_gender_output_acc: 0.7102 - val_image_quality_output_acc: 0.6964 - val_age_output_acc: 0.8006 - val_weight_output_acc: 0.8206 - val_bag_output_acc: 0.7082 - val_footwear_output_acc: 0.7428 - val_pose_output_acc: 0.7858 - val_emotion_output_acc: 0.8640\n",
            "\n",
            "Epoch 00028: val_loss improved from 5.78213 to 5.71512, saving model to /content/saved_models/PersonAttribute%s_model.028.h5\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.6347 - gender_output_loss: 0.5344 - image_quality_output_loss: 0.5409 - age_output_loss: 0.4464 - weight_output_loss: 0.4134 - bag_output_loss: 0.5190 - footwear_output_loss: 0.5134 - pose_output_loss: 0.4539 - emotion_output_loss: 0.3769 - gender_output_acc: 0.7276 - image_quality_output_acc: 0.7143 - age_output_acc: 0.8002 - weight_output_acc: 0.8191 - bag_output_acc: 0.7349 - footwear_output_acc: 0.7461 - pose_output_acc: 0.7879 - emotion_output_acc: 0.8546 - val_loss: 5.7940 - val_gender_output_loss: 0.6420 - val_image_quality_output_loss: 0.5635 - val_age_output_loss: 0.4462 - val_weight_output_loss: 0.4140 - val_bag_output_loss: 0.5400 - val_footwear_output_loss: 0.5212 - val_pose_output_loss: 0.4498 - val_emotion_output_loss: 0.3672 - val_gender_output_acc: 0.6734 - val_image_quality_output_acc: 0.6971 - val_age_output_acc: 0.8010 - val_weight_output_acc: 0.8159 - val_bag_output_acc: 0.7157 - val_footwear_output_acc: 0.7473 - val_pose_output_acc: 0.7932 - val_emotion_output_acc: 0.8625\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 5.71512\n",
            "Epoch 30/50\n",
            "360/360 [==============================] - 188s 523ms/step - loss: 5.6322 - gender_output_loss: 0.5347 - image_quality_output_loss: 0.5419 - age_output_loss: 0.4461 - weight_output_loss: 0.4125 - bag_output_loss: 0.5186 - footwear_output_loss: 0.5141 - pose_output_loss: 0.4523 - emotion_output_loss: 0.3770 - gender_output_acc: 0.7272 - image_quality_output_acc: 0.7156 - age_output_acc: 0.7999 - weight_output_acc: 0.8189 - bag_output_acc: 0.7351 - footwear_output_acc: 0.7429 - pose_output_acc: 0.7908 - emotion_output_acc: 0.8544 - val_loss: 5.6871 - val_gender_output_loss: 0.5755 - val_image_quality_output_loss: 0.5616 - val_age_output_loss: 0.4421 - val_weight_output_loss: 0.4107 - val_bag_output_loss: 0.5292 - val_footwear_output_loss: 0.5284 - val_pose_output_loss: 0.4424 - val_emotion_output_loss: 0.3639 - val_gender_output_acc: 0.6809 - val_image_quality_output_acc: 0.7011 - val_age_output_acc: 0.8012 - val_weight_output_acc: 0.8217 - val_bag_output_acc: 0.7181 - val_footwear_output_acc: 0.7325 - val_pose_output_acc: 0.7969 - val_emotion_output_acc: 0.8639\n",
            "\n",
            "Epoch 00030: val_loss improved from 5.71512 to 5.68710, saving model to /content/saved_models/PersonAttribute%s_model.030.h5\n",
            "Epoch 31/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.6099 - gender_output_loss: 0.5306 - image_quality_output_loss: 0.5414 - age_output_loss: 0.4466 - weight_output_loss: 0.4125 - bag_output_loss: 0.5164 - footwear_output_loss: 0.5078 - pose_output_loss: 0.4478 - emotion_output_loss: 0.3766 - gender_output_acc: 0.7280 - image_quality_output_acc: 0.7145 - age_output_acc: 0.7999 - weight_output_acc: 0.8191 - bag_output_acc: 0.7354 - footwear_output_acc: 0.7493 - pose_output_acc: 0.7933 - emotion_output_acc: 0.8543 - val_loss: 5.6699 - val_gender_output_loss: 0.5474 - val_image_quality_output_loss: 0.5639 - val_age_output_loss: 0.4427 - val_weight_output_loss: 0.4117 - val_bag_output_loss: 0.5276 - val_footwear_output_loss: 0.5171 - val_pose_output_loss: 0.4498 - val_emotion_output_loss: 0.3649 - val_gender_output_acc: 0.7235 - val_image_quality_output_acc: 0.6991 - val_age_output_acc: 0.8015 - val_weight_output_acc: 0.8214 - val_bag_output_acc: 0.7270 - val_footwear_output_acc: 0.7520 - val_pose_output_acc: 0.7932 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00031: val_loss improved from 5.68710 to 5.66988, saving model to /content/saved_models/PersonAttribute%s_model.031.h5\n",
            "Epoch 32/50\n",
            "360/360 [==============================] - 187s 521ms/step - loss: 5.5991 - gender_output_loss: 0.5249 - image_quality_output_loss: 0.5410 - age_output_loss: 0.4459 - weight_output_loss: 0.4124 - bag_output_loss: 0.5167 - footwear_output_loss: 0.5094 - pose_output_loss: 0.4456 - emotion_output_loss: 0.3763 - gender_output_acc: 0.7324 - image_quality_output_acc: 0.7173 - age_output_acc: 0.8000 - weight_output_acc: 0.8192 - bag_output_acc: 0.7386 - footwear_output_acc: 0.7489 - pose_output_acc: 0.7934 - emotion_output_acc: 0.8544 - val_loss: 6.0179 - val_gender_output_loss: 0.6273 - val_image_quality_output_loss: 0.5747 - val_age_output_loss: 0.4407 - val_weight_output_loss: 0.4118 - val_bag_output_loss: 0.5363 - val_footwear_output_loss: 0.5176 - val_pose_output_loss: 0.5609 - val_emotion_output_loss: 0.3777 - val_gender_output_acc: 0.6724 - val_image_quality_output_acc: 0.6828 - val_age_output_acc: 0.8011 - val_weight_output_acc: 0.8226 - val_bag_output_acc: 0.7132 - val_footwear_output_acc: 0.7438 - val_pose_output_acc: 0.7584 - val_emotion_output_acc: 0.8604\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 5.66988\n",
            "Epoch 33/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.5793 - gender_output_loss: 0.5212 - image_quality_output_loss: 0.5397 - age_output_loss: 0.4457 - weight_output_loss: 0.4119 - bag_output_loss: 0.5165 - footwear_output_loss: 0.5102 - pose_output_loss: 0.4395 - emotion_output_loss: 0.3760 - gender_output_acc: 0.7374 - image_quality_output_acc: 0.7145 - age_output_acc: 0.8006 - weight_output_acc: 0.8186 - bag_output_acc: 0.7336 - footwear_output_acc: 0.7475 - pose_output_acc: 0.7974 - emotion_output_acc: 0.8547\n",
            "Epoch 00032: val_loss did not improve from 5.66988\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.5791 - gender_output_loss: 0.5211 - image_quality_output_loss: 0.5396 - age_output_loss: 0.4457 - weight_output_loss: 0.4119 - bag_output_loss: 0.5165 - footwear_output_loss: 0.5103 - pose_output_loss: 0.4394 - emotion_output_loss: 0.3760 - gender_output_acc: 0.7374 - image_quality_output_acc: 0.7145 - age_output_acc: 0.8005 - weight_output_acc: 0.8187 - bag_output_acc: 0.7336 - footwear_output_acc: 0.7474 - pose_output_acc: 0.7974 - emotion_output_acc: 0.8547 - val_loss: 5.6801 - val_gender_output_loss: 0.5466 - val_image_quality_output_loss: 0.5698 - val_age_output_loss: 0.4421 - val_weight_output_loss: 0.4112 - val_bag_output_loss: 0.5260 - val_footwear_output_loss: 0.5164 - val_pose_output_loss: 0.4506 - val_emotion_output_loss: 0.3661 - val_gender_output_acc: 0.7162 - val_image_quality_output_acc: 0.6956 - val_age_output_acc: 0.8028 - val_weight_output_acc: 0.8193 - val_bag_output_acc: 0.7256 - val_footwear_output_acc: 0.7441 - val_pose_output_acc: 0.7949 - val_emotion_output_acc: 0.8645\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 5.66988\n",
            "Epoch 34/50\n",
            "360/360 [==============================] - 188s 521ms/step - loss: 5.5791 - gender_output_loss: 0.5211 - image_quality_output_loss: 0.5396 - age_output_loss: 0.4457 - weight_output_loss: 0.4119 - bag_output_loss: 0.5165 - footwear_output_loss: 0.5103 - pose_output_loss: 0.4394 - emotion_output_loss: 0.3760 - gender_output_acc: 0.7374 - image_quality_output_acc: 0.7145 - age_output_acc: 0.8005 - weight_output_acc: 0.8187 - bag_output_acc: 0.7336 - footwear_output_acc: 0.7474 - pose_output_acc: 0.7974 - emotion_output_acc: 0.8547 - val_loss: 5.6801 - val_gender_output_loss: 0.5466 - val_image_quality_output_loss: 0.5698 - val_age_output_loss: 0.4421 - val_weight_output_loss: 0.4112 - val_bag_output_loss: 0.5260 - val_footwear_output_loss: 0.5164 - val_pose_output_loss: 0.4506 - val_emotion_output_loss: 0.3661 - val_gender_output_acc: 0.7162 - val_image_quality_output_acc: 0.6956 - val_age_output_acc: 0.8028 - val_weight_output_acc: 0.8193 - val_bag_output_acc: 0.7256 - val_footwear_output_acc: 0.7441 - val_pose_output_acc: 0.7949 - val_emotion_output_acc: 0.8645\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 5.5670 - gender_output_loss: 0.5180 - image_quality_output_loss: 0.5410 - age_output_loss: 0.4459 - weight_output_loss: 0.4121 - bag_output_loss: 0.5128 - footwear_output_loss: 0.5112 - pose_output_loss: 0.4348 - emotion_output_loss: 0.3757 - gender_output_acc: 0.7388 - image_quality_output_acc: 0.7176 - age_output_acc: 0.8000 - weight_output_acc: 0.8195 - bag_output_acc: 0.7410 - footwear_output_acc: 0.7455 - pose_output_acc: 0.7986 - emotion_output_acc: 0.8544 - val_loss: 5.6974 - val_gender_output_loss: 0.5181 - val_image_quality_output_loss: 0.5677 - val_age_output_loss: 0.4414 - val_weight_output_loss: 0.4103 - val_bag_output_loss: 0.5275 - val_footwear_output_loss: 0.5156 - val_pose_output_loss: 0.4777 - val_emotion_output_loss: 0.3648 - val_gender_output_acc: 0.7349 - val_image_quality_output_acc: 0.6976 - val_age_output_acc: 0.8019 - val_weight_output_acc: 0.8208 - val_bag_output_acc: 0.7275 - val_footwear_output_acc: 0.7476 - val_pose_output_acc: 0.7639 - val_emotion_output_acc: 0.8640\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 5.66988\n",
            "Epoch 35/50\n",
            "360/360 [==============================] - 188s 522ms/step - loss: 5.5535 - gender_output_loss: 0.5140 - image_quality_output_loss: 0.5386 - age_output_loss: 0.4456 - weight_output_loss: 0.4123 - bag_output_loss: 0.5159 - footwear_output_loss: 0.5076 - pose_output_loss: 0.4328 - emotion_output_loss: 0.3756 - gender_output_acc: 0.7431 - image_quality_output_acc: 0.7176 - age_output_acc: 0.7998 - weight_output_acc: 0.8190 - bag_output_acc: 0.7359 - footwear_output_acc: 0.7503 - pose_output_acc: 0.7997 - emotion_output_acc: 0.8544 - val_loss: 5.6606 - val_gender_output_loss: 0.5165 - val_image_quality_output_loss: 0.5600 - val_age_output_loss: 0.4433 - val_weight_output_loss: 0.4117 - val_bag_output_loss: 0.5262 - val_footwear_output_loss: 0.5378 - val_pose_output_loss: 0.4541 - val_emotion_output_loss: 0.3651 - val_gender_output_acc: 0.7364 - val_image_quality_output_acc: 0.7033 - val_age_output_acc: 0.7998 - val_weight_output_acc: 0.8201 - val_bag_output_acc: 0.7231 - val_footwear_output_acc: 0.7347 - val_pose_output_acc: 0.7878 - val_emotion_output_acc: 0.8639\n",
            "\n",
            "Epoch 00035: val_loss improved from 5.66988 to 5.66055, saving model to /content/saved_models/PersonAttribute%s_model.035.h5\n",
            "Epoch 36/50\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 5.5499 - gender_output_loss: 0.5112 - image_quality_output_loss: 0.5401 - age_output_loss: 0.4455 - weight_output_loss: 0.4128 - bag_output_loss: 0.5154 - footwear_output_loss: 0.5068 - pose_output_loss: 0.4316 - emotion_output_loss: 0.3753 - gender_output_acc: 0.7432 - image_quality_output_acc: 0.7148 - age_output_acc: 0.8001 - weight_output_acc: 0.8187 - bag_output_acc: 0.7387 - footwear_output_acc: 0.7512 - pose_output_acc: 0.8015 - emotion_output_acc: 0.8546 - val_loss: 5.6364 - val_gender_output_loss: 0.5539 - val_image_quality_output_loss: 0.5643 - val_age_output_loss: 0.4427 - val_weight_output_loss: 0.4116 - val_bag_output_loss: 0.5329 - val_footwear_output_loss: 0.5087 - val_pose_output_loss: 0.4323 - val_emotion_output_loss: 0.3632 - val_gender_output_acc: 0.7122 - val_image_quality_output_acc: 0.6994 - val_age_output_acc: 0.8006 - val_weight_output_acc: 0.8217 - val_bag_output_acc: 0.7196 - val_footwear_output_acc: 0.7517 - val_pose_output_acc: 0.8101 - val_emotion_output_acc: 0.8645\n",
            "\n",
            "Epoch 00036: val_loss improved from 5.66055 to 5.63644, saving model to /content/saved_models/PersonAttribute%s_model.036.h5\n",
            "Epoch 37/50\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 5.5426 - gender_output_loss: 0.5134 - image_quality_output_loss: 0.5381 - age_output_loss: 0.4457 - weight_output_loss: 0.4121 - bag_output_loss: 0.5154 - footwear_output_loss: 0.5056 - pose_output_loss: 0.4298 - emotion_output_loss: 0.3752 - gender_output_acc: 0.7453 - image_quality_output_acc: 0.7157 - age_output_acc: 0.7996 - weight_output_acc: 0.8189 - bag_output_acc: 0.7378 - footwear_output_acc: 0.7511 - pose_output_acc: 0.8016 - emotion_output_acc: 0.8544 - val_loss: 5.6516 - val_gender_output_loss: 0.5224 - val_image_quality_output_loss: 0.5695 - val_age_output_loss: 0.4421 - val_weight_output_loss: 0.4105 - val_bag_output_loss: 0.5257 - val_footwear_output_loss: 0.5098 - val_pose_output_loss: 0.4545 - val_emotion_output_loss: 0.3638 - val_gender_output_acc: 0.7394 - val_image_quality_output_acc: 0.6872 - val_age_output_acc: 0.8007 - val_weight_output_acc: 0.8214 - val_bag_output_acc: 0.7287 - val_footwear_output_acc: 0.7530 - val_pose_output_acc: 0.7870 - val_emotion_output_acc: 0.8640\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 5.63644\n",
            "Epoch 38/50\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 5.5241 - gender_output_loss: 0.5077 - image_quality_output_loss: 0.5389 - age_output_loss: 0.4448 - weight_output_loss: 0.4112 - bag_output_loss: 0.5136 - footwear_output_loss: 0.5051 - pose_output_loss: 0.4263 - emotion_output_loss: 0.3737 - gender_output_acc: 0.7454 - image_quality_output_acc: 0.7177 - age_output_acc: 0.8002 - weight_output_acc: 0.8192 - bag_output_acc: 0.7395 - footwear_output_acc: 0.7491 - pose_output_acc: 0.8033 - emotion_output_acc: 0.8547 - val_loss: 5.6197 - val_gender_output_loss: 0.5443 - val_image_quality_output_loss: 0.5581 - val_age_output_loss: 0.4440 - val_weight_output_loss: 0.4113 - val_bag_output_loss: 0.5300 - val_footwear_output_loss: 0.5205 - val_pose_output_loss: 0.4290 - val_emotion_output_loss: 0.3639 - val_gender_output_acc: 0.7200 - val_image_quality_output_acc: 0.7018 - val_age_output_acc: 0.8006 - val_weight_output_acc: 0.8203 - val_bag_output_acc: 0.7270 - val_footwear_output_acc: 0.7465 - val_pose_output_acc: 0.8053 - val_emotion_output_acc: 0.8638\n",
            "\n",
            "Epoch 00038: val_loss improved from 5.63644 to 5.61970, saving model to /content/saved_models/PersonAttribute%s_model.038.h5\n",
            "Epoch 39/50\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 5.5054 - gender_output_loss: 0.5035 - image_quality_output_loss: 0.5393 - age_output_loss: 0.4450 - weight_output_loss: 0.4116 - bag_output_loss: 0.5141 - footwear_output_loss: 0.5037 - pose_output_loss: 0.4182 - emotion_output_loss: 0.3745 - gender_output_acc: 0.7494 - image_quality_output_acc: 0.7179 - age_output_acc: 0.7998 - weight_output_acc: 0.8192 - bag_output_acc: 0.7399 - footwear_output_acc: 0.7519 - pose_output_acc: 0.8075 - emotion_output_acc: 0.8547 - val_loss: 5.5565 - val_gender_output_loss: 0.5009 - val_image_quality_output_loss: 0.5564 - val_age_output_loss: 0.4414 - val_weight_output_loss: 0.4104 - val_bag_output_loss: 0.5249 - val_footwear_output_loss: 0.5146 - val_pose_output_loss: 0.4303 - val_emotion_output_loss: 0.3628 - val_gender_output_acc: 0.7495 - val_image_quality_output_acc: 0.7014 - val_age_output_acc: 0.8017 - val_weight_output_acc: 0.8197 - val_bag_output_acc: 0.7261 - val_footwear_output_acc: 0.7503 - val_pose_output_acc: 0.8071 - val_emotion_output_acc: 0.8639\n",
            "\n",
            "Epoch 00039: val_loss improved from 5.61970 to 5.55646, saving model to /content/saved_models/PersonAttribute%s_model.039.h5\n",
            "Epoch 40/50\n",
            "360/360 [==============================] - 186s 517ms/step - loss: 5.4949 - gender_output_loss: 0.4984 - image_quality_output_loss: 0.5368 - age_output_loss: 0.4452 - weight_output_loss: 0.4115 - bag_output_loss: 0.5131 - footwear_output_loss: 0.5030 - pose_output_loss: 0.4185 - emotion_output_loss: 0.3748 - gender_output_acc: 0.7556 - image_quality_output_acc: 0.7164 - age_output_acc: 0.7999 - weight_output_acc: 0.8191 - bag_output_acc: 0.7395 - footwear_output_acc: 0.7506 - pose_output_acc: 0.8085 - emotion_output_acc: 0.8545 - val_loss: 5.7912 - val_gender_output_loss: 0.5799 - val_image_quality_output_loss: 0.5815 - val_age_output_loss: 0.4437 - val_weight_output_loss: 0.4098 - val_bag_output_loss: 0.5326 - val_footwear_output_loss: 0.5120 - val_pose_output_loss: 0.4774 - val_emotion_output_loss: 0.3644 - val_gender_output_acc: 0.6981 - val_image_quality_output_acc: 0.6845 - val_age_output_acc: 0.8020 - val_weight_output_acc: 0.8217 - val_bag_output_acc: 0.7219 - val_footwear_output_acc: 0.7470 - val_pose_output_acc: 0.7717 - val_emotion_output_acc: 0.8639\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 5.55646\n",
            "Epoch 41/50\n",
            "360/360 [==============================] - 186s 518ms/step - loss: 5.4726 - gender_output_loss: 0.4905 - image_quality_output_loss: 0.5386 - age_output_loss: 0.4448 - weight_output_loss: 0.4119 - bag_output_loss: 0.5115 - footwear_output_loss: 0.5034 - pose_output_loss: 0.4113 - emotion_output_loss: 0.3734 - gender_output_acc: 0.7587 - image_quality_output_acc: 0.7169 - age_output_acc: 0.8002 - weight_output_acc: 0.8195 - bag_output_acc: 0.7402 - footwear_output_acc: 0.7527 - pose_output_acc: 0.8113 - emotion_output_acc: 0.8546 - val_loss: 5.7911 - val_gender_output_loss: 0.5118 - val_image_quality_output_loss: 0.6333 - val_age_output_loss: 0.4432 - val_weight_output_loss: 0.4121 - val_bag_output_loss: 0.5245 - val_footwear_output_loss: 0.5260 - val_pose_output_loss: 0.4522 - val_emotion_output_loss: 0.3689 - val_gender_output_acc: 0.7520 - val_image_quality_output_acc: 0.6573 - val_age_output_acc: 0.8008 - val_weight_output_acc: 0.8207 - val_bag_output_acc: 0.7268 - val_footwear_output_acc: 0.7376 - val_pose_output_acc: 0.8007 - val_emotion_output_acc: 0.8643\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 5.55646\n",
            "Epoch 42/50\n",
            "360/360 [==============================] - 186s 518ms/step - loss: 5.4748 - gender_output_loss: 0.4914 - image_quality_output_loss: 0.5362 - age_output_loss: 0.4448 - weight_output_loss: 0.4115 - bag_output_loss: 0.5125 - footwear_output_loss: 0.5012 - pose_output_loss: 0.4148 - emotion_output_loss: 0.3738 - gender_output_acc: 0.7543 - image_quality_output_acc: 0.7191 - age_output_acc: 0.7999 - weight_output_acc: 0.8190 - bag_output_acc: 0.7419 - footwear_output_acc: 0.7536 - pose_output_acc: 0.8108 - emotion_output_acc: 0.8545 - val_loss: 5.5417 - val_gender_output_loss: 0.5069 - val_image_quality_output_loss: 0.5565 - val_age_output_loss: 0.4415 - val_weight_output_loss: 0.4100 - val_bag_output_loss: 0.5229 - val_footwear_output_loss: 0.5078 - val_pose_output_loss: 0.4250 - val_emotion_output_loss: 0.3621 - val_gender_output_acc: 0.7525 - val_image_quality_output_acc: 0.7028 - val_age_output_acc: 0.8002 - val_weight_output_acc: 0.8223 - val_bag_output_acc: 0.7298 - val_footwear_output_acc: 0.7555 - val_pose_output_acc: 0.8046 - val_emotion_output_acc: 0.8642\n",
            "\n",
            "Epoch 00042: val_loss improved from 5.55646 to 5.54165, saving model to /content/saved_models/PersonAttribute%s_model.042.h5\n",
            "Epoch 43/50\n",
            "360/360 [==============================] - 187s 519ms/step - loss: 5.4699 - gender_output_loss: 0.4881 - image_quality_output_loss: 0.5376 - age_output_loss: 0.4447 - weight_output_loss: 0.4121 - bag_output_loss: 0.5143 - footwear_output_loss: 0.4999 - pose_output_loss: 0.4122 - emotion_output_loss: 0.3735 - gender_output_acc: 0.7596 - image_quality_output_acc: 0.7159 - age_output_acc: 0.7999 - weight_output_acc: 0.8193 - bag_output_acc: 0.7400 - footwear_output_acc: 0.7553 - pose_output_acc: 0.8123 - emotion_output_acc: 0.8545 - val_loss: 5.8597 - val_gender_output_loss: 0.7636 - val_image_quality_output_loss: 0.5664 - val_age_output_loss: 0.4464 - val_weight_output_loss: 0.4107 - val_bag_output_loss: 0.5409 - val_footwear_output_loss: 0.5167 - val_pose_output_loss: 0.4245 - val_emotion_output_loss: 0.3653 - val_gender_output_acc: 0.6071 - val_image_quality_output_acc: 0.6883 - val_age_output_acc: 0.7996 - val_weight_output_acc: 0.8204 - val_bag_output_acc: 0.7085 - val_footwear_output_acc: 0.7418 - val_pose_output_acc: 0.8076 - val_emotion_output_acc: 0.8643\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 5.54165\n",
            "Epoch 44/50\n",
            "325/360 [==========================>...] - ETA: 17s - loss: 5.4512 - gender_output_loss: 0.4867 - image_quality_output_loss: 0.5370 - age_output_loss: 0.4432 - weight_output_loss: 0.4099 - bag_output_loss: 0.5125 - footwear_output_loss: 0.4994 - pose_output_loss: 0.4101 - emotion_output_loss: 0.3716 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.7177 - age_output_acc: 0.8007 - weight_output_acc: 0.8206 - bag_output_acc: 0.7424 - footwear_output_acc: 0.7538 - pose_output_acc: 0.8133 - emotion_output_acc: 0.8556"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:610: UserWarning: The input 49 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 221s 613ms/step - loss: 5.4556 - gender_output_loss: 0.4885 - image_quality_output_loss: 0.5369 - age_output_loss: 0.4439 - weight_output_loss: 0.4109 - bag_output_loss: 0.5114 - footwear_output_loss: 0.5000 - pose_output_loss: 0.4089 - emotion_output_loss: 0.3732 - gender_output_acc: 0.7557 - image_quality_output_acc: 0.7180 - age_output_acc: 0.8006 - weight_output_acc: 0.8199 - bag_output_acc: 0.7429 - footwear_output_acc: 0.7538 - pose_output_acc: 0.8142 - emotion_output_acc: 0.8546 - val_loss: 5.5691 - val_gender_output_loss: 0.5085 - val_image_quality_output_loss: 0.5601 - val_age_output_loss: 0.4407 - val_weight_output_loss: 0.4098 - val_bag_output_loss: 0.5237 - val_footwear_output_loss: 0.5144 - val_pose_output_loss: 0.4304 - val_emotion_output_loss: 0.3636 - val_gender_output_acc: 0.7412 - val_image_quality_output_acc: 0.7011 - val_age_output_acc: 0.8017 - val_weight_output_acc: 0.8206 - val_bag_output_acc: 0.7270 - val_footwear_output_acc: 0.7490 - val_pose_output_acc: 0.8090 - val_emotion_output_acc: 0.8628\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 5.54165\n",
            "Epoch 45/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-746:\n",
            "Process ForkPoolWorker-755:\n",
            "Process ForkPoolWorker-748:\n",
            "Process ForkPoolWorker-754:\n",
            "Process ForkPoolWorker-745:\n",
            "KeyboardInterrupt\n",
            "Process ForkPoolWorker-747:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-b522dad05f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}